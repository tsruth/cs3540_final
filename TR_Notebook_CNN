{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":604.816764,"end_time":"2024-03-21T13:19:27.943484","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-03-21T13:09:23.12672","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import polars as pl\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder,LabelEncoder\nfrom sklearn.metrics import roc_auc_score \nfrom sklearn.utils import shuffle\nimport warnings\nfrom sklearn.model_selection import GroupKFold, StratifiedGroupKFold\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.feature_extraction import FeatureHasher\nfrom tensorflow import keras\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras import optimizers\n\n\ndataPath = \"/kaggle/input/home-credit-credit-risk-model-stability/\"","metadata":{"_kg_hide-input":false,"papermill":{"duration":4.273401,"end_time":"2024-03-21T13:09:30.582633","exception":false,"start_time":"2024-03-21T13:09:26.309232","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-30T20:29:43.309356Z","iopub.execute_input":"2024-04-30T20:29:43.310129Z","iopub.status.idle":"2024-04-30T20:30:08.250315Z","shell.execute_reply.started":"2024-04-30T20:29:43.310098Z","shell.execute_reply":"2024-04-30T20:30:08.248801Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n  warnings.warn(\n2024-04-30 20:29:58.277181: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-30 20:29:58.277292: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-30 20:29:58.554829: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load Dataset\n\nThis is given code used to load in the data frome kaggle.","metadata":{}},{"cell_type":"code","source":"def set_table_dtypes(df: pl.DataFrame) -> pl.DataFrame:\n    # implement here all desired dtypes for tables\n    # the following is just an example\n    for col in df.columns:\n        # last letter of column name will help you determine the type\n        if col[-1] in (\"P\", \"A\"):\n                df = df.with_columns(pl.col(col).cast(pl.Float64))\n    return df\n\ndef convert_strings(df: pd.DataFrame) -> pd.DataFrame: \n    for col in df.columns:  \n        if df[col].dtype.name in ['object', 'string']:\n            df[col] = df[col].astype(\"string\").astype('category')\n            current_categories = df[col].cat.categories\n            new_categories = current_categories.to_list() + [\"Unknown\"]\n            new_dtype = pd.CategoricalDtype(categories=new_categories, ordered=True)\n            df[col] = df[col].astype(new_dtype)\n    return df\n\n#https://www.kaggle.com/code/darynarr/home-credit-drop-date-features/notebook\ndef reduce_memory_usage_pl(df):\n        \"\"\" Reduce memory usage by polars dataframe {df} with name {name} by changing its data types.\n            Original pandas version of this function: https://www.kaggle.com/code/arjanso/reducing-dataframe-memory-size-by-65 \"\"\"\n        print(f\"Memory usage of dataframe is {round(df.estimated_size('mb'), 2)} MB\")\n        Numeric_Int_types = [pl.Int8,pl.Int16,pl.Int32,pl.Int64]\n        Numeric_Float_types = [pl.Float32,pl.Float64]    \n        for col in df.columns:\n            try:\n                col_type = df[col].dtype\n                if col_type == pl.Categorical:\n                    continue\n                c_min = df[col].min()\n                c_max = df[col].max()\n                if col_type in Numeric_Int_types:\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df = df.with_columns(df[col].cast(pl.Int32))\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df = df.with_columns(df[col].cast(pl.Int16))\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df = df.with_columns(df[col].cast(pl.Int32))\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df = df.with_columns(df[col].cast(pl.Int64))\n                elif col_type in Numeric_Float_types:\n                    if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df = df.with_columns(df[col].cast(pl.Float32))\n                    else:\n                        pass\n                # elif col_type == pl.Utf8:\n                #     df = df.with_columns(df[col].cast(pl.Categorical))\n                else:\n                    pass\n            except:\n                pass\n        print(f\"Memory usage of dataframe became {round(df.estimated_size('mb'), 2)} MB\")\n        return df","metadata":{"papermill":{"duration":0.028347,"end_time":"2024-03-21T13:09:30.616411","exception":false,"start_time":"2024-03-21T13:09:30.588064","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-30T20:30:08.252532Z","iopub.execute_input":"2024-04-30T20:30:08.253305Z","iopub.status.idle":"2024-04-30T20:30:08.269749Z","shell.execute_reply.started":"2024-04-30T20:30:08.253274Z","shell.execute_reply":"2024-04-30T20:30:08.268854Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_basetable = pl.read_csv(dataPath+'csv_files/train/train_base.csv').pipe(set_table_dtypes)\n\ntrain_static = pl.concat(\n[pl.read_csv(dataPath + \"csv_files/train/train_static_0_0.csv\").pipe(set_table_dtypes),\npl.read_csv(dataPath + \"csv_files/train/train_static_0_1.csv\").pipe(set_table_dtypes)\n],how = 'vertical_relaxed',\n)\n\ntrain_applprev = pl.concat(\n[pl.read_csv(dataPath + \"csv_files/train/train_applprev_1_0.csv\").pipe(set_table_dtypes),\npl.read_csv(dataPath + \"csv_files/train/train_applprev_1_1.csv\").pipe(set_table_dtypes)   \n],how = 'vertical_relaxed',\n)\n\ntrain_static_cb = pl.read_csv(dataPath + \"csv_files/train/train_static_cb_0.csv\").pipe(set_table_dtypes)\ntrain_person_1 = pl.read_csv(dataPath + \"csv_files/train/train_person_1.csv\").pipe(set_table_dtypes)\ntrain_credit_bureau_a_2_5 = pl.read_csv(dataPath + \"csv_files/train/train_credit_bureau_a_2_5.csv\").pipe(set_table_dtypes)\ntrain_deposit_1 = pl.read_csv(dataPath + \"csv_files/train/train_deposit_1.csv\").pipe(set_table_dtypes)\n","metadata":{"_kg_hide-input":true,"papermill":{"duration":45.036287,"end_time":"2024-03-21T13:10:15.657778","exception":false,"start_time":"2024-03-21T13:09:30.621491","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-30T20:30:08.271040Z","iopub.execute_input":"2024-04-30T20:30:08.271357Z","iopub.status.idle":"2024-04-30T20:30:47.395048Z","shell.execute_reply.started":"2024-04-30T20:30:08.271333Z","shell.execute_reply":"2024-04-30T20:30:47.394154Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"test_basetable = pl.read_csv(dataPath+'csv_files/test/test_base.csv').pipe(set_table_dtypes)\n\ntest_static = pl.concat(\n[pl.read_csv(dataPath + \"csv_files/test/test_static_0_0.csv\").pipe(set_table_dtypes),\npl.read_csv(dataPath + \"csv_files/test/test_static_0_1.csv\").pipe(set_table_dtypes)\n],how = 'vertical_relaxed',\n)\ntest_applprev = pl.concat(\n[       pl.read_csv(dataPath + \"csv_files/test/test_applprev_1_0.csv\").pipe(set_table_dtypes),\n        pl.read_csv(dataPath + \"csv_files/test/test_applprev_1_1.csv\").pipe(set_table_dtypes),    \n],how = 'vertical_relaxed',\n)\n\ntest_static_cb = pl.read_csv(dataPath + \"csv_files/test/test_static_cb_0.csv\").pipe(set_table_dtypes)\ntest_person_1 = pl.read_csv(dataPath + \"csv_files/test/test_person_1.csv\").pipe(set_table_dtypes)\ntest_credit_bureau_a_2_5 = pl.read_csv(dataPath + \"csv_files/test/test_credit_bureau_a_2_5.csv\").pipe(set_table_dtypes)\ntest_deposit_1 = pl.read_csv(dataPath + \"csv_files/test/test_deposit_1.csv\").pipe(set_table_dtypes)","metadata":{"papermill":{"duration":0.097894,"end_time":"2024-03-21T13:10:15.761027","exception":false,"start_time":"2024-03-21T13:10:15.663133","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-30T20:30:47.397395Z","iopub.execute_input":"2024-04-30T20:30:47.397731Z","iopub.status.idle":"2024-04-30T20:30:47.489584Z","shell.execute_reply.started":"2024-04-30T20:30:47.397704Z","shell.execute_reply":"2024-04-30T20:30:47.488646Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class FeatureEngineer:\n    def __init__(self, train_basetable, train_static, train_static_cb,\n                 train_person_1, train_credit_bureau_a_2_5,train_deposit_1,train_applprev):\n        \n        self.train_basetable = train_basetable\n        self.train_static = train_static\n        self.train_static_cb = train_static_cb\n        self.train_person_1 = train_person_1\n        self.train_credit_bureau_a_2_5 = train_credit_bureau_a_2_5\n        self.train_deposit_1 = train_deposit_1\n        self.train_applprev = train_applprev\n        \n        \n    def generate_person_features(self):\n        # Aggregating features from train_person_1\n        train_person_1_feats_1 = self.train_person_1.group_by(\"case_id\").agg(\n            pl.col(\"mainoccupationinc_384A\").max().alias(\"mainoccupationinc_384A_maxA\"),\n            (pl.col(\"incometype_1044T\") == \"SELFEMPLOYED\").max().alias(\"mainoccupationinc_384A_any_selfemployedA\"),\n            pl.col(\"childnum_185L\").sum().alias(\"total_childrenL\").cast(pl.Int32)\n        )\n        \n        # Filtering and selecting features from train_person_1\n        train_person_1_feats_2 = self.train_person_1.select([\"case_id\", \"num_group1\", \"housetype_905L\"]).filter(\n            pl.col(\"num_group1\") == 0).drop(\"num_group1\").rename({\"housetype_905L\": \"person_housetypeL\"})\n        \n        \n        return train_person_1_feats_1, train_person_1_feats_2\n    \n    \n    def deposit_frequency(self):\n        deposit_frequency = self.train_deposit_1.group_by(\"case_id\").agg(\n        pl.count(\"amount_416A\").alias(\"deposit_countL\")\n    )   \n        return deposit_frequency\n    \n    \n    def generate_applprev(self):\n         # Total Payment Amount\n            total_payment_amount = self.train_applprev.group_by(\"case_id\").agg(\n                pl.col(\"pmtnum_8L\").sum().alias(\"total_payment_amountL\"))\n            \n            # average Annuity\n            avg_annuity = self.train_applprev.group_by(\"case_id\").agg(\n                pl.col(\"annuity_853A\").mean().alias(\"avg_annuityA\"))\n\n            \n            #payment_rate = self.train_applprev.with_columns(\n                #(pl.col(\"annuity_853A\")/pl.col(\"credamount_590A\")).alias(\"payment_rateL\"))\n\n            return total_payment_amount,avg_annuity #payment_rate\n\n           \n    \n    def generate_static_columns(self, df):\n        selected_static_cols = [col for col in df.columns if col[-1] in (\"A\", \"M\")]\n        return df.select([\"case_id\"] + selected_static_cols) \n     \n \n        # Generating features\n    def join_tables(self):\n        person_feats_1, person_feats_2 = self.generate_person_features()\n        deposit_frequency = self.deposit_frequency()\n        total_payment_amount,avg_annuity = self.generate_applprev()\n         \n        # Selecting static columns\n        selected_static = self.generate_static_columns(self.train_static)\n        selected_static_cb = self.generate_static_columns(self.train_static_cb)\n        \n        #joining data\n        data = self.train_basetable.join(selected_static, how=\"left\", on=\"case_id\") \\\n                                   .join(selected_static_cb, how=\"left\", on=\"case_id\") \\\n                                   .join(person_feats_1, how=\"left\", on=\"case_id\") \\\n                                   .join(person_feats_2, how=\"left\", on=\"case_id\") \\\n                                   .join(deposit_frequency, how=\"left\", on=\"case_id\") \\\n                                   .join(total_payment_amount, how=\"left\", on=\"case_id\") \\\n                                   .join(avg_annuity, how=\"left\", on=\"case_id\") \n\n                                   #.join(payment_rate, how=\"left\", on=\"case_id\") \n\n        data = data.with_columns(\n            pl.col(pl.Float64).cast(pl.Float32)\n        )\n        \n        \n        return data\n    \n            \n       # Generate and join features for the test set\n    def process_test_set(self, test_basetable, test_static, test_static_cb, test_person_1, test_credit_bureau_a_2_5, test_deposit_1,test_applprev):\n        fe_test = FeatureEngineer(test_basetable, test_static, test_static_cb, test_person_1, test_credit_bureau_a_2_5, test_deposit_1,test_applprev)\n        \n        test_data = fe_test.join_tables()\n\n\n        return test_data\n    \n\nfe = FeatureEngineer(train_basetable, train_static, train_static_cb, train_person_1,\n                     train_credit_bureau_a_2_5,train_deposit_1,train_applprev)\n\ndata = fe.join_tables()\ndata = reduce_memory_usage_pl(data)\n\n\ntest_data = fe.process_test_set(test_basetable, test_static, test_static_cb,\n                                test_person_1, test_credit_bureau_a_2_5, test_deposit_1,test_applprev)\n\ntest_data = reduce_memory_usage_pl(test_data)","metadata":{"papermill":{"duration":4.482152,"end_time":"2024-03-21T13:10:20.248373","exception":false,"start_time":"2024-03-21T13:10:15.766221","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-30T20:51:02.758335Z","iopub.execute_input":"2024-04-30T20:51:02.759204Z","iopub.status.idle":"2024-04-30T20:51:06.523751Z","shell.execute_reply.started":"2024-04-30T20:51:02.759168Z","shell.execute_reply":"2024-04-30T20:51:06.521376Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"Memory usage of dataframe is 635.99 MB\nMemory usage of dataframe became 612.69 MB\nMemory usage of dataframe is 0.0 MB\nMemory usage of dataframe became 0.0 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"case_ids = data[\"case_id\"].unique().shuffle(seed=1)\ncase_ids_train, case_ids_test = train_test_split(case_ids, train_size=0.6, random_state=42)\ncase_ids_valid, case_ids_test = train_test_split(case_ids_test, train_size=0.5, random_state=42)\n\ncols_pred = []\nfor col in data.columns:\n    if col[-1].isupper() and col[:-1].islower():\n        cols_pred.append(col)\n\n        \ndef from_polars_to_pandas(case_ids: pl.DataFrame) -> pl.DataFrame:\n    return (\n        data.filter(pl.col(\"case_id\").is_in(case_ids))[[\"case_id\", \"WEEK_NUM\", \"target\"]].to_pandas(),\n        data.filter(pl.col(\"case_id\").is_in(case_ids))[cols_pred].to_pandas(),\n        data.filter(pl.col(\"case_id\").is_in(case_ids))[\"target\"].to_pandas()\n    )\n\nbase_train, X_train, y_train = from_polars_to_pandas(case_ids_train)\nbase_valid, X_valid, y_valid = from_polars_to_pandas(case_ids_valid)\nbase_test, X_test, y_test = from_polars_to_pandas(case_ids_test)\n\n","metadata":{"_kg_hide-output":false,"papermill":{"duration":9.118823,"end_time":"2024-03-21T13:10:29.372353","exception":false,"start_time":"2024-03-21T13:10:20.25353","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-30T20:51:06.528628Z","iopub.execute_input":"2024-04-30T20:51:06.529098Z","iopub.status.idle":"2024-04-30T20:51:09.501092Z","shell.execute_reply.started":"2024-04-30T20:51:06.529015Z","shell.execute_reply":"2024-04-30T20:51:09.499847Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"def col_hasher(df: pd.DataFrame, cols: [str]) -> pd.DataFrame:\n    \"\"\"\n    Hashes a list of dataframe columns, modifies pandas chained_assignment warning to suppress false positives\n    \n    @params:\n        df:  Pandas DataFrame, REQUIRED\n        cols: List of strings representing the columns in the dataframe\n        \n    @returns:\n        df: Pandas DataFrame with hashed columns\n    \"\"\"\n    \n    pd.options.mode.chained_assignment = None\n    for col in cols:\n        hashed = pd.util.hash_array(np.array(df[col]))\n        df[col] = hashed\n        \n    pd.reset_option(\"mode.chained_assignment\")\n    return df\n\n\ndef gen_synthetic(x_data: pd.DataFrame, n: int, y_data: pd.Series, syn_type: int):\n    \"\"\"\n    x: X_train\n    n: number of entries to generate\n    y: y_train\n    syn_type: class to generate, either 0 or 1\n    \"\"\"\n    \n    \n    x_data = x_data.assign(target=y_data.values)\n    x_data = x_data[x_data['target'] == syn_type]\n    x_data = x_data.drop('target', axis = 1)\n    \n    syn_y = []\n\n    syn_data = {}\n    for col in x_data.columns.to_list():\n        syn_data[col] = []\n        data = x_data[col].value_counts().index.to_list()\n        marginal = list(marginals(x_data, col).values())\n        synthetic = np.random.choice(data, size=n, p=marginal)\n        \n        for syn_data_point in synthetic:\n            syn_data[col].append(syn_data_point)\n    \n    for i in range(n):\n        syn_y.append(1)\n        \n    syn_x = pd.DataFrame.from_dict(syn_data)\n    syn_y = pd.Series(syn_y)\n    return syn_x, syn_y\n\ndef marginals(df: pd.DataFrame, col: str) -> {}:\n    \"\"\"\n    maps the probabilty of an occurence to the occurence\n    \"\"\"\n    data = df[col].value_counts()\n    results = [x for x in data]\n    labels = df[col].value_counts().index.to_list()\n    syn_rep = {}\n    \n    for x in range(len(labels)):\n        syn_rep[labels[x]] = max(0, results[x])\n    \n    total = sum(syn_rep.values())\n    \n    marginal = {}\n    for x in labels:\n        marginal[x] = syn_rep[x] / total\n    return marginal\n\ndef preprocess_data(x: pd.DataFrame, y: pd.DataFrame, synth_class: int) -> pd.DataFrame:\n    '''\n    executes preprocessing as one function for ease of use\n    \n    @param\n    x: data to preprocess\n    y: target data for synthetic\n    synthetic: whether or not to enhance data\n    synth_class: which class to enhance\n    '''\n\n    # FILL CATEGORICAL FEATURE NAN VALUES\n    index = x.dtypes.index.to_list()\n    categoricals = {}\n        \n    for i in range(len(x.dtypes)):\n    \n        if x.dtypes.iloc[i] == \"float32\" or x.dtypes.iloc[i] == \"float64\" or x.dtypes.iloc[i] == \"int32\":\n            continue\n        else:\n            categoricals[index[i]] = str(x.dtypes.iloc[i])\n\n            data = x[index[i]].value_counts().index.to_list()\n\n            x = x.assign(**{index[i]:x[index[i]].fillna(data[0])})\n            \n    for col in x.columns:\n        if x[col].isna().sum() > 0:\n            x = x.assign(**{col:x[col].fillna(x[col].mean())})\n    \n    syn_x, syn_y = gen_synthetic(x_data = x, n = len(x), y_data = y, syn_type = synth_class)\n\n    syn_X_train = pd.concat([syn_x, x])\n    syn_y_train = pd.concat([syn_y, y])\n\n    # Merge and shuffle data\n    syn_X_train['target'] = syn_y_train\n    syn_X_train = syn_X_train.sample(frac = 1)\n\n    syn_y_train = syn_X_train['target']\n    syn_X_train = syn_X_train.drop('target', axis = 1)\n\n    # FEATURE TYPE CONVERSION\n    float64_cols = list(syn_X_train.select_dtypes(include='float64'))\n\n    # The same code again calling the columns\n    syn_X_train[float64_cols] = syn_X_train[float64_cols].astype('float32')\n\n    encoder = OrdinalEncoder()\n    encoder.fit(syn_X_train[categoricals.keys()])\n\n    syn_X_train[list(categoricals.keys())] = encoder.transform(syn_X_train[list(categoricals.keys())])\n\n    normalize_syn_x = syn_X_train.copy()\n\n    # apply normalization techniques \n    for column in normalize_syn_x.columns: \n        normalize_syn_x[column] = (normalize_syn_x[column] - normalize_syn_x[column].min()) / (normalize_syn_x[column].max() - normalize_syn_x[column].min())\n\n    return normalize_syn_x, syn_y_train\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T20:51:09.502625Z","iopub.execute_input":"2024-04-30T20:51:09.502988Z","iopub.status.idle":"2024-04-30T20:51:09.526309Z","shell.execute_reply.started":"2024-04-30T20:51:09.502961Z","shell.execute_reply":"2024-04-30T20:51:09.525240Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"## Establish Categorical Columns","metadata":{}},{"cell_type":"code","source":"x_train, y_train = preprocess_data(X_train.head(int(len(X_train) / 2)), y_train.head(int(len(y_train) / 2)), 1)\nx_valid, y_valid = preprocess_data(X_valid.head(int(len(X_valid) / 2)), y_valid.head(int(len(y_valid) / 2)), 1)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T20:51:33.187665Z","iopub.execute_input":"2024-04-30T20:51:33.188462Z","iopub.status.idle":"2024-04-30T20:52:32.183736Z","shell.execute_reply.started":"2024-04-30T20:51:33.188424Z","shell.execute_reply":"2024-04-30T20:52:32.182197Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"y_train.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-30T20:47:51.672097Z","iopub.execute_input":"2024-04-30T20:47:51.672479Z","iopub.status.idle":"2024-04-30T20:47:51.687242Z","shell.execute_reply.started":"2024-04-30T20:47:51.672443Z","shell.execute_reply":"2024-04-30T20:47:51.686334Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"target\n1    471800\n0    444194\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"X_train:{X_train.shape}\")\nprint(f\"Valid: {X_valid.shape}\")\nprint(f\"Test: {X_test.shape}\")","metadata":{"papermill":{"duration":0.015898,"end_time":"2024-03-21T13:10:29.393533","exception":false,"start_time":"2024-03-21T13:10:29.377635","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-30T19:02:03.104878Z","iopub.execute_input":"2024-04-30T19:02:03.105288Z","iopub.status.idle":"2024-04-30T19:02:03.110744Z","shell.execute_reply.started":"2024-04-30T19:02:03.105258Z","shell.execute_reply":"2024-04-30T19:02:03.109534Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"X_train:(915995, 53)\nValid: (305332, 53)\nTest: (305332, 53)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-04-30T19:02:19.984829Z","iopub.execute_input":"2024-04-30T19:02:19.985153Z","iopub.status.idle":"2024-04-30T19:02:19.989632Z","shell.execute_reply.started":"2024-04-30T19:02:19.985124Z","shell.execute_reply":"2024-04-30T19:02:19.988597Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Different Training Sets\n- X_train: standard data\n- xtrain: standard data with NaN values filled with 0\n- syn_X_train: standard data supplemented with synthetic data to help with the imbalance of target distributions\n- normalize_syn_x: syn_X_train but with values normalized","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import optimizers\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nlr_schedule = keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate = 0.05,\n    decay_steps=2000,\n    decay_rate=0.95,\n    staircase=True)\n\nann = models.Sequential([\n    layers.Dense(1024, activation='relu', input_shape = (None, 53)),\n    layers.Dense(1024, activation='relu'),\n    layers.Dense(1024, activation='relu'),\n    layers.Dense(1024, activation='relu'),\n    layers.Dense(1024, activation='relu'),\n    layers.Dense(1024, activation='relu'),\n    layers.Dense(1024, activation='relu'),\n    layers.Dense(1024, activation='relu'),\n    layers.Dense(1024, activation='relu'),\n    layers.Dense(1024, activation='relu'),\n    layers.Dense(1, activation='softmax')\n])\n\nann.summary()\n\n#refer to https://www.tensorflow.org/tutorials/keras/classification tutorial to check on how to use compile function\nann.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss='binary_crossentropy', metrics=['accuracy'])\n\n# This will start the training and save each epoch output in the history list.\nhistory_ann = ann.fit(normalize_syn_x, syn_y_train, batch_size=16, epochs=5)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T19:45:17.917895Z","iopub.execute_input":"2024-04-30T19:45:17.918269Z","iopub.status.idle":"2024-04-30T19:45:30.166053Z","shell.execute_reply.started":"2024-04-30T19:45:17.918226Z","shell.execute_reply":"2024-04-30T19:45:30.165291Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stdout","text":"Model: \"sequential_32\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_145 (Dense)           (None, None, 1024)        55296     \n                                                                 \n dense_146 (Dense)           (None, None, 1024)        1049600   \n                                                                 \n dense_147 (Dense)           (None, None, 1024)        1049600   \n                                                                 \n dense_148 (Dense)           (None, None, 1024)        1049600   \n                                                                 \n dense_149 (Dense)           (None, None, 1024)        1049600   \n                                                                 \n dense_150 (Dense)           (None, None, 1024)        1049600   \n                                                                 \n dense_151 (Dense)           (None, None, 1024)        1049600   \n                                                                 \n dense_152 (Dense)           (None, None, 1024)        1049600   \n                                                                 \n dense_153 (Dense)           (None, None, 1024)        1049600   \n                                                                 \n dense_154 (Dense)           (None, None, 1024)        1049600   \n                                                                 \n dense_155 (Dense)           (None, None, 1)           1025      \n                                                                 \n=================================================================\nTotal params: 9502721 (36.25 MB)\nTrainable params: 9502721 (36.25 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nEpoch 1/5\n313/313 [==============================] - 5s 6ms/step - loss: 314239.5000 - accuracy: 0.5133\nEpoch 2/5\n313/313 [==============================] - 2s 6ms/step - loss: 0.7120 - accuracy: 0.5133\nEpoch 3/5\n313/313 [==============================] - 2s 6ms/step - loss: 0.7076 - accuracy: 0.5133\nEpoch 4/5\n313/313 [==============================] - 2s 6ms/step - loss: 0.7056 - accuracy: 0.5133\nEpoch 5/5\n313/313 [==============================] - 2s 6ms/step - loss: 0.7064 - accuracy: 0.5133\n","output_type":"stream"}]},{"cell_type":"code","source":"ann.predict(syn_X_train.tail(10))","metadata":{"execution":{"iopub.status.busy":"2024-04-30T19:45:49.008499Z","iopub.execute_input":"2024-04-30T19:45:49.009221Z","iopub.status.idle":"2024-04-30T19:45:49.172774Z","shell.execute_reply.started":"2024-04-30T19:45:49.009189Z","shell.execute_reply":"2024-04-30T19:45:49.171909Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 102ms/step\n","output_type":"stream"},{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"array([[1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import optimizers\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nlr_schedule = keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate = 0.0001,\n    decay_steps=20000,\n    decay_rate=0.95,\n    staircase=True)\n\ncnn = models.Sequential([\n    layers.Conv1D(filters = 256, kernel_size = 21, activation='relu', padding = 'same', input_shape=(53, 1)),\n    layers.Conv1D(filters = 256, kernel_size = 17, activation='relu', padding = 'same'),\n    layers.MaxPooling1D(1, padding = 'same'),\n    layers.Dropout(0.2),\n    layers.Conv1D(filters = 256, kernel_size = 17, activation='relu', padding = 'same'),\n    layers.Conv1D(filters = 512, kernel_size = 12, activation='relu', padding = 'same'),\n    layers.MaxPooling1D(1, padding = 'same'),\n    layers.Flatten(),\n    layers.Dense(1024, activation='sigmoid'),\n    layers.Dense(1, activation='sigmoid')\n])\n\ncnn.summary()\n\n\n#refer to https://www.tensorflow.org/tutorials/keras/classification tutorial to check on how to use compile function\ncnn.compile(optimizer='rmsprop', loss = 'binary_crossentropy', metrics=['accuracy'])\n\n# This will start the training and save each epoch output in the history list.\nhistory_cnn = cnn.fit(x_train.head(25000), y_train.head(25000), batch_size=128, epochs=4, validation_data=(x_valid, y_valid))","metadata":{"execution":{"iopub.status.busy":"2024-04-30T20:53:54.974008Z","iopub.execute_input":"2024-04-30T20:53:54.974457Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Model: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv1d_8 (Conv1D)           (None, 53, 256)           5632      \n                                                                 \n conv1d_9 (Conv1D)           (None, 53, 256)           1114368   \n                                                                 \n max_pooling1d_4 (MaxPoolin  (None, 53, 256)           0         \n g1D)                                                            \n                                                                 \n dropout_2 (Dropout)         (None, 53, 256)           0         \n                                                                 \n conv1d_10 (Conv1D)          (None, 53, 256)           1114368   \n                                                                 \n conv1d_11 (Conv1D)          (None, 53, 512)           1573376   \n                                                                 \n max_pooling1d_5 (MaxPoolin  (None, 53, 512)           0         \n g1D)                                                            \n                                                                 \n flatten_2 (Flatten)         (None, 27136)             0         \n                                                                 \n dense_4 (Dense)             (None, 1024)              27788288  \n                                                                 \n dense_5 (Dense)             (None, 1)                 1025      \n                                                                 \n=================================================================\nTotal params: 31597057 (120.53 MB)\nTrainable params: 31597057 (120.53 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nEpoch 1/4\n196/196 [==============================] - 38s 186ms/step - loss: 0.7035 - accuracy: 0.5078 - val_loss: 0.6980 - val_accuracy: 0.4852\nEpoch 2/4\n196/196 [==============================] - 34s 175ms/step - loss: 0.6715 - accuracy: 0.5891 - val_loss: 0.6409 - val_accuracy: 0.6468\nEpoch 3/4\n196/196 [==============================] - 34s 175ms/step - loss: 0.5489 - accuracy: 0.7225 - val_loss: 0.7002 - val_accuracy: 0.5571\nEpoch 4/4\n 63/196 [========>.....................] - ETA: 6s - loss: 0.4784 - accuracy: 0.7657","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-04-30T20:41:36.008563Z","iopub.execute_input":"2024-04-30T20:41:36.008985Z","iopub.status.idle":"2024-04-30T20:41:36.022637Z","shell.execute_reply.started":"2024-04-30T20:41:36.008952Z","shell.execute_reply":"2024-04-30T20:41:36.021618Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"array([1])"},"metadata":{}}]},{"cell_type":"code","source":"cnn.predict(x_train.tail(100))","metadata":{"execution":{"iopub.status.busy":"2024-04-30T20:48:54.776547Z","iopub.execute_input":"2024-04-30T20:48:54.777262Z","iopub.status.idle":"2024-04-30T20:48:54.981654Z","shell.execute_reply.started":"2024-04-30T20:48:54.777228Z","shell.execute_reply":"2024-04-30T20:48:54.980663Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"4/4 [==============================] - 0s 5ms/step\n","output_type":"stream"},{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"array([[0.53402513],\n       [0.5335739 ],\n       [0.6420369 ],\n       [0.8959239 ],\n       [0.11769948],\n       [0.11550926],\n       [0.6597783 ],\n       [0.8488924 ],\n       [0.28921407],\n       [0.07344264],\n       [0.4013049 ],\n       [0.4239103 ],\n       [0.7416972 ],\n       [0.82160896],\n       [0.01879908],\n       [0.01426143],\n       [0.48801097],\n       [0.7192755 ],\n       [0.84219795],\n       [0.92362523],\n       [0.01992249],\n       [0.13514894],\n       [0.07778189],\n       [0.68553275],\n       [0.7712308 ],\n       [0.9896382 ],\n       [0.1224228 ],\n       [0.98843867],\n       [0.705541  ],\n       [0.7691645 ],\n       [0.5629969 ],\n       [0.49503538],\n       [0.9887383 ],\n       [0.12053678],\n       [0.08463439],\n       [0.9111283 ],\n       [0.03435889],\n       [0.7747206 ],\n       [0.02189592],\n       [0.01834733],\n       [0.9940303 ],\n       [0.3964402 ],\n       [0.6385386 ],\n       [0.0824187 ],\n       [0.01881852],\n       [0.01341291],\n       [0.8621142 ],\n       [0.19773665],\n       [0.25961682],\n       [0.25035077],\n       [0.63482815],\n       [0.0203643 ],\n       [0.08587395],\n       [0.20452532],\n       [0.29771948],\n       [0.9911499 ],\n       [0.38693702],\n       [0.06642488],\n       [0.7443206 ],\n       [0.9863349 ],\n       [0.42482543],\n       [0.01603168],\n       [0.7252781 ],\n       [0.7862811 ],\n       [0.8076022 ],\n       [0.40218982],\n       [0.9192132 ],\n       [0.10605773],\n       [0.3754364 ],\n       [0.01996764],\n       [0.18258905],\n       [0.9929931 ],\n       [0.01840518],\n       [0.03219999],\n       [0.33226013],\n       [0.20259884],\n       [0.15790859],\n       [0.06761392],\n       [0.8366399 ],\n       [0.5867073 ],\n       [0.8966299 ],\n       [0.05178823],\n       [0.06318275],\n       [0.0247047 ],\n       [0.01919218],\n       [0.03879556],\n       [0.48207587],\n       [0.24370173],\n       [0.3856016 ],\n       [0.81953716],\n       [0.09060889],\n       [0.5402113 ],\n       [0.02687562],\n       [0.19199437],\n       [0.53567845],\n       [0.5162597 ],\n       [0.6220381 ],\n       [0.10788674],\n       [0.24961385],\n       [0.7353503 ]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}