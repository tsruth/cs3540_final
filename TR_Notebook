{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":604.816764,"end_time":"2024-03-21T13:19:27.943484","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-03-21T13:09:23.12672","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import polars as pl\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder,LabelEncoder\nfrom sklearn.metrics import roc_auc_score \nimport warnings\nfrom sklearn.model_selection import GroupKFold, StratifiedGroupKFold\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.feature_extraction import FeatureHasher\nfrom tensorflow import keras\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras import optimizers\n\n\ndataPath = \"/kaggle/input/home-credit-credit-risk-model-stability/\"","metadata":{"_kg_hide-input":false,"papermill":{"duration":4.273401,"end_time":"2024-03-21T13:09:30.582633","exception":false,"start_time":"2024-03-21T13:09:26.309232","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_table_dtypes(df: pl.DataFrame) -> pl.DataFrame:\n    # implement here all desired dtypes for tables\n    # the following is just an example\n    for col in df.columns:\n        # last letter of column name will help you determine the type\n        if col[-1] in (\"P\", \"A\"):\n                df = df.with_columns(pl.col(col).cast(pl.Float64))\n    return df\n\ndef convert_strings(df: pd.DataFrame) -> pd.DataFrame: \n    for col in df.columns:  \n        if df[col].dtype.name in ['object', 'string']:\n            df[col] = df[col].astype(\"string\").astype('category')\n            current_categories = df[col].cat.categories\n            new_categories = current_categories.to_list() + [\"Unknown\"]\n            new_dtype = pd.CategoricalDtype(categories=new_categories, ordered=True)\n            df[col] = df[col].astype(new_dtype)\n    return df\n\n#https://www.kaggle.com/code/darynarr/home-credit-drop-date-features/notebook\ndef reduce_memory_usage_pl(df):\n        \"\"\" Reduce memory usage by polars dataframe {df} with name {name} by changing its data types.\n            Original pandas version of this function: https://www.kaggle.com/code/arjanso/reducing-dataframe-memory-size-by-65 \"\"\"\n        print(f\"Memory usage of dataframe is {round(df.estimated_size('mb'), 2)} MB\")\n        Numeric_Int_types = [pl.Int8,pl.Int16,pl.Int32,pl.Int64]\n        Numeric_Float_types = [pl.Float32,pl.Float64]    \n        for col in df.columns:\n            try:\n                col_type = df[col].dtype\n                if col_type == pl.Categorical:\n                    continue\n                c_min = df[col].min()\n                c_max = df[col].max()\n                if col_type in Numeric_Int_types:\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df = df.with_columns(df[col].cast(pl.Int32))\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df = df.with_columns(df[col].cast(pl.Int16))\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df = df.with_columns(df[col].cast(pl.Int32))\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df = df.with_columns(df[col].cast(pl.Int64))\n                elif col_type in Numeric_Float_types:\n                    if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df = df.with_columns(df[col].cast(pl.Float32))\n                    else:\n                        pass\n                # elif col_type == pl.Utf8:\n                #     df = df.with_columns(df[col].cast(pl.Categorical))\n                else:\n                    pass\n            except:\n                pass\n        print(f\"Memory usage of dataframe became {round(df.estimated_size('mb'), 2)} MB\")\n        return df","metadata":{"papermill":{"duration":0.028347,"end_time":"2024-03-21T13:09:30.616411","exception":false,"start_time":"2024-03-21T13:09:30.588064","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_basetable = pl.read_csv(dataPath+'csv_files/train/train_base.csv').pipe(set_table_dtypes)\n\ntrain_static = pl.concat(\n[pl.read_csv(dataPath + \"csv_files/train/train_static_0_0.csv\").pipe(set_table_dtypes),\npl.read_csv(dataPath + \"csv_files/train/train_static_0_1.csv\").pipe(set_table_dtypes)\n],how = 'vertical_relaxed',\n)\n\ntrain_applprev = pl.concat(\n[pl.read_csv(dataPath + \"csv_files/train/train_applprev_1_0.csv\").pipe(set_table_dtypes),\npl.read_csv(dataPath + \"csv_files/train/train_applprev_1_1.csv\").pipe(set_table_dtypes)   \n],how = 'vertical_relaxed',\n)\n\ntrain_static_cb = pl.read_csv(dataPath + \"csv_files/train/train_static_cb_0.csv\").pipe(set_table_dtypes)\ntrain_person_1 = pl.read_csv(dataPath + \"csv_files/train/train_person_1.csv\").pipe(set_table_dtypes)\ntrain_credit_bureau_a_2_5 = pl.read_csv(dataPath + \"csv_files/train/train_credit_bureau_a_2_5.csv\").pipe(set_table_dtypes)\ntrain_deposit_1 = pl.read_csv(dataPath + \"csv_files/train/train_deposit_1.csv\").pipe(set_table_dtypes)\n","metadata":{"_kg_hide-input":true,"papermill":{"duration":45.036287,"end_time":"2024-03-21T13:10:15.657778","exception":false,"start_time":"2024-03-21T13:09:30.621491","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_basetable = pl.read_csv(dataPath+'csv_files/test/test_base.csv').pipe(set_table_dtypes)\n\ntest_static = pl.concat(\n[pl.read_csv(dataPath + \"csv_files/test/test_static_0_0.csv\").pipe(set_table_dtypes),\npl.read_csv(dataPath + \"csv_files/test/test_static_0_1.csv\").pipe(set_table_dtypes)\n],how = 'vertical_relaxed',\n)\ntest_applprev = pl.concat(\n[       pl.read_csv(dataPath + \"csv_files/test/test_applprev_1_0.csv\").pipe(set_table_dtypes),\n        pl.read_csv(dataPath + \"csv_files/test/test_applprev_1_1.csv\").pipe(set_table_dtypes),    \n],how = 'vertical_relaxed',\n)\n\ntest_static_cb = pl.read_csv(dataPath + \"csv_files/test/test_static_cb_0.csv\").pipe(set_table_dtypes)\ntest_person_1 = pl.read_csv(dataPath + \"csv_files/test/test_person_1.csv\").pipe(set_table_dtypes)\ntest_credit_bureau_a_2_5 = pl.read_csv(dataPath + \"csv_files/test/test_credit_bureau_a_2_5.csv\").pipe(set_table_dtypes)\ntest_deposit_1 = pl.read_csv(dataPath + \"csv_files/test/test_deposit_1.csv\").pipe(set_table_dtypes)","metadata":{"papermill":{"duration":0.097894,"end_time":"2024-03-21T13:10:15.761027","exception":false,"start_time":"2024-03-21T13:10:15.663133","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeatureEngineer:\n    def __init__(self, train_basetable, train_static, train_static_cb,\n                 train_person_1, train_credit_bureau_a_2_5,train_deposit_1,train_applprev):\n        \n        self.train_basetable = train_basetable\n        self.train_static = train_static\n        self.train_static_cb = train_static_cb\n        self.train_person_1 = train_person_1\n        self.train_credit_bureau_a_2_5 = train_credit_bureau_a_2_5\n        self.train_deposit_1 = train_deposit_1\n        self.train_applprev = train_applprev\n        \n        \n    def generate_person_features(self):\n        # Aggregating features from train_person_1\n        train_person_1_feats_1 = self.train_person_1.group_by(\"case_id\").agg(\n            pl.col(\"mainoccupationinc_384A\").max().alias(\"mainoccupationinc_384A_maxA\"),\n            (pl.col(\"incometype_1044T\") == \"SELFEMPLOYED\").max().alias(\"mainoccupationinc_384A_any_selfemployedA\"),\n            pl.col(\"childnum_185L\").sum().alias(\"total_childrenL\").cast(pl.Int32)\n        )\n        \n        # Filtering and selecting features from train_person_1\n        train_person_1_feats_2 = self.train_person_1.select([\"case_id\", \"num_group1\", \"housetype_905L\"]).filter(\n            pl.col(\"num_group1\") == 0).drop(\"num_group1\").rename({\"housetype_905L\": \"person_housetypeL\"})\n        \n        \n        return train_person_1_feats_1, train_person_1_feats_2\n    \n    \n    def deposit_frequency(self):\n        deposit_frequency = self.train_deposit_1.group_by(\"case_id\").agg(\n        pl.count(\"amount_416A\").alias(\"deposit_countL\")\n    )   \n        return deposit_frequency\n    \n    \n    def generate_applprev(self):\n         # Total Payment Amount\n            total_payment_amount = self.train_applprev.group_by(\"case_id\").agg(\n                pl.col(\"pmtnum_8L\").sum().alias(\"total_payment_amountL\"))\n            \n            # average Annuity\n            avg_annuity = self.train_applprev.group_by(\"case_id\").agg(\n                pl.col(\"annuity_853A\").mean().alias(\"avg_annuityA\"))\n\n            \n            #payment_rate = self.train_applprev.with_columns(\n                #(pl.col(\"annuity_853A\")/pl.col(\"credamount_590A\")).alias(\"payment_rateL\"))\n\n            return total_payment_amount,avg_annuity #payment_rate\n\n           \n    \n    def generate_static_columns(self, df):\n        selected_static_cols = [col for col in df.columns if col[-1] in (\"A\", \"M\")]\n        return df.select([\"case_id\"] + selected_static_cols) \n     \n \n        # Generating features\n    def join_tables(self):\n        person_feats_1, person_feats_2 = self.generate_person_features()\n        deposit_frequency = self.deposit_frequency()\n        total_payment_amount,avg_annuity = self.generate_applprev()\n         \n        # Selecting static columns\n        selected_static = self.generate_static_columns(self.train_static)\n        selected_static_cb = self.generate_static_columns(self.train_static_cb)\n        \n        #joining data\n        data = self.train_basetable.join(selected_static, how=\"left\", on=\"case_id\") \\\n                                   .join(selected_static_cb, how=\"left\", on=\"case_id\") \\\n                                   .join(person_feats_1, how=\"left\", on=\"case_id\") \\\n                                   .join(person_feats_2, how=\"left\", on=\"case_id\") \\\n                                   .join(deposit_frequency, how=\"left\", on=\"case_id\") \\\n                                   .join(total_payment_amount, how=\"left\", on=\"case_id\") \\\n                                   .join(avg_annuity, how=\"left\", on=\"case_id\") \n\n                                   #.join(payment_rate, how=\"left\", on=\"case_id\") \n\n        data = data.with_columns(\n            pl.col(pl.Float64).cast(pl.Float32)\n        )\n        \n        \n        return data\n    \n            \n       # Generate and join features for the test set\n    def process_test_set(self, test_basetable, test_static, test_static_cb, test_person_1, test_credit_bureau_a_2_5, test_deposit_1,test_applprev):\n        fe_test = FeatureEngineer(test_basetable, test_static, test_static_cb, test_person_1, test_credit_bureau_a_2_5, test_deposit_1,test_applprev)\n        \n        test_data = fe_test.join_tables()\n\n\n        return test_data\n    \n\nfe = FeatureEngineer(train_basetable, train_static, train_static_cb, train_person_1,\n                     train_credit_bureau_a_2_5,train_deposit_1,train_applprev)\n\ndata = fe.join_tables()\ndata = reduce_memory_usage_pl(data)\n\n\ntest_data = fe.process_test_set(test_basetable, test_static, test_static_cb,\n                                test_person_1, test_credit_bureau_a_2_5, test_deposit_1,test_applprev)\n\ntest_data = reduce_memory_usage_pl(test_data)","metadata":{"papermill":{"duration":4.482152,"end_time":"2024-03-21T13:10:20.248373","exception":false,"start_time":"2024-03-21T13:10:15.766221","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"case_ids = data[\"case_id\"].unique().shuffle(seed=1)\ncase_ids_train, case_ids_test = train_test_split(case_ids, train_size=0.6, random_state=42)\ncase_ids_valid, case_ids_test = train_test_split(case_ids_test, train_size=0.5, random_state=42)\n\ncols_pred = []\nfor col in data.columns:\n    if col[-1].isupper() and col[:-1].islower():\n        cols_pred.append(col)\n\n        \ndef from_polars_to_pandas(case_ids: pl.DataFrame) -> pl.DataFrame:\n    return (\n        data.filter(pl.col(\"case_id\").is_in(case_ids))[[\"case_id\", \"WEEK_NUM\", \"target\"]].to_pandas(),\n        data.filter(pl.col(\"case_id\").is_in(case_ids))[cols_pred].to_pandas(),\n        data.filter(pl.col(\"case_id\").is_in(case_ids))[\"target\"].to_pandas()\n    )\n\nbase_train, X_train, y_train = from_polars_to_pandas(case_ids_train)\nbase_valid, X_valid, y_valid = from_polars_to_pandas(case_ids_valid)\nbase_test, X_test, y_test = from_polars_to_pandas(case_ids_test)\n\nfor df in [X_train, X_valid, X_test]:\n    df = convert_strings(df)","metadata":{"_kg_hide-output":false,"papermill":{"duration":9.118823,"end_time":"2024-03-21T13:10:29.372353","exception":false,"start_time":"2024-03-21T13:10:20.25353","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"X_train:{X_train.shape}\")\nprint(f\"Valid: {X_valid.shape}\")\nprint(f\"Test: {X_test.shape}\")","metadata":{"papermill":{"duration":0.015898,"end_time":"2024-03-21T13:10:29.393533","exception":false,"start_time":"2024-03-21T13:10:29.377635","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def col_hasher(df: pd.DataFrame, cols: [str] = list(df.columns)) -> pd.DataFrame:\n    \"\"\"\n    Hashes a list of dataframe columns, modifies pandas chained_assignment warning to suppress false positives\n    \n    @params:\n        df:  Pandas DataFrame, REQUIRED\n        cols: List of strings representing the columns in the dataframe, default to all\n        \n    @returns:\n        df: Pandas DataFrame with hashed columns\n    \"\"\"\n    \n    pd.options.mode.chained_assignment = None\n    for col in cols:\n        hashed = pd.util.hash_array(np.array(df[col]))\n        df[col] = hashed\n        \n    pd.reset_option(\"mode.chained_assignment\")\n    return df\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ohe(X):\n    encoder = OneHotEncoder()\n    encoder.fit(demo)\n    transformed = encoder.transform(demo)\n    ohe_df = pd.DataFrame(transformed.toarray())\n    X = ohe_df\n    return X\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hashed_X_train = col_hasher(X_train)\nhashed_X_valid = col_hasher(X_valid)\nhashed_X_test = col_hasher(X_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"layers = [\n    Flatten(),\n    Dense(100,\n          activation = \"relu\"),\n    Dense(100,\n          activation = \"relu\"),\n    Dense(100,\n          activation = \"relu\"),\n    Dense(100,\n          activation = \"relu\"),\n    Dense(100,\n          activation = \"relu\"),\n    Dense(100,\n          activation = \"relu\"),\n    Dense(100,\n          activation = \"relu\"),\n    Dense(100,\n          activation = \"relu\"),\n    Dense(10,\n          activation = \"softmax\")\n]\n\nmodel = Sequential(layers)\n\n#refer to https://www.tensorflow.org/tutorials/keras/classification tutorial to check on how to use compile function\nmodel.compile(optimizer='SGD', loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n\n# This will start the training and save each epoch output in the history list.\nhistory = model.fit(X_train, y_train, batch_size=128, epochs=10 , validation_data=(X_valid, y_valid))","metadata":{"papermill":{"duration":0.013378,"end_time":"2024-03-21T13:19:25.105451","exception":false,"start_time":"2024-03-21T13:19:25.092073","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(history.history['accuracy'], label=\"acc\")\nplt.plot(history.history['val_accuracy'], label=\"val_acc\")\nplt.legend()\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-11T01:23:28.903614Z","iopub.status.idle":"2024-04-11T01:23:28.904045Z","shell.execute_reply.started":"2024-04-11T01:23:28.903810Z","shell.execute_reply":"2024-04-11T01:23:28.903827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T20:34:44.146010Z","iopub.execute_input":"2024-04-10T20:34:44.146349Z","iopub.status.idle":"2024-04-10T20:34:44.178866Z","shell.execute_reply.started":"2024-04-10T20:34:44.146304Z","shell.execute_reply":"2024-04-10T20:34:44.177948Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Model: \"sequential_6\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten_6 (Flatten)         (None, 53)                0         \n                                                                 \n dense_50 (Dense)            (None, 100)               5400      \n                                                                 \n dense_51 (Dense)            (None, 100)               10100     \n                                                                 \n dense_52 (Dense)            (None, 100)               10100     \n                                                                 \n dense_53 (Dense)            (None, 100)               10100     \n                                                                 \n dense_54 (Dense)            (None, 100)               10100     \n                                                                 \n dense_55 (Dense)            (None, 100)               10100     \n                                                                 \n dense_56 (Dense)            (None, 100)               10100     \n                                                                 \n dense_57 (Dense)            (None, 100)               10100     \n                                                                 \n dense_58 (Dense)            (None, 10)                1010      \n                                                                 \n=================================================================\nTotal params: 77110 (301.21 KB)\nTrainable params: 77110 (301.21 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"columns_to_use = ['avgoutstandbalancel6m_4187114A', 'downpmt_116A', \n           'lastrejectcredamount_222A', 'lastrejectreason_759M',  'maxannuity_159A', 'totaldebt_9A', \n            'totinstallast1m_4525188A', 'education_88M']\n\nfiltered_X_train = hashed_X_train[columns_to_use]\nfiltered_X_valid = hashed_X_valid[columns_to_use]\nfiltered_X_test = hashed_X_test[columns_to_use]","metadata":{"execution":{"iopub.status.busy":"2024-04-11T14:11:18.444291Z","iopub.execute_input":"2024-04-11T14:11:18.444966Z","iopub.status.idle":"2024-04-11T14:11:18.555938Z","shell.execute_reply.started":"2024-04-11T14:11:18.444932Z","shell.execute_reply":"2024-04-11T14:11:18.555031Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"layers = [\n    Flatten(),\n    Dense(100,\n          activation = \"relu\"),\n    Dense(100,\n          activation = \"relu\"),\n    Dense(100,\n          activation = \"relu\"),\n    Dense(100,\n          activation = \"relu\"),\n    Dense(100,\n          activation = \"relu\"),\n    Dense(100,\n          activation = \"relu\"),\n    Dense(100,\n          activation = \"relu\"),\n    Dense(100,\n          activation = \"relu\"),\n    Dense(10,\n          activation = \"softmax\")\n]\n\nmodel = Sequential(layers)\n\n#refer to https://www.tensorflow.org/tutorials/keras/classification tutorial to check on how to use compile function\nmodel.compile(optimizer='SGD', loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n\n# This will start the training and save each epoch output in the history list.\nhistory = model.fit(filtered_X_train, y_train, batch_size=256, epochs=4 , validation_data=(filtered_X_valid, y_valid))","metadata":{"execution":{"iopub.status.busy":"2024-04-10T21:10:01.766273Z","iopub.execute_input":"2024-04-10T21:10:01.766705Z","iopub.status.idle":"2024-04-10T21:15:36.530511Z","shell.execute_reply.started":"2024-04-10T21:10:01.766674Z","shell.execute_reply":"2024-04-10T21:15:36.529572Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1712783404.834209     318 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"7157/7157 [==============================] - 36s 5ms/step - loss: nan - accuracy: 0.9683 - val_loss: nan - val_accuracy: 0.9686\nEpoch 2/10\n7157/7157 [==============================] - 33s 5ms/step - loss: nan - accuracy: 0.9684 - val_loss: nan - val_accuracy: 0.9686\nEpoch 3/10\n7157/7157 [==============================] - 33s 5ms/step - loss: nan - accuracy: 0.9684 - val_loss: nan - val_accuracy: 0.9686\nEpoch 4/10\n7157/7157 [==============================] - 33s 5ms/step - loss: nan - accuracy: 0.9684 - val_loss: nan - val_accuracy: 0.9686\nEpoch 5/10\n7157/7157 [==============================] - 33s 5ms/step - loss: nan - accuracy: 0.9684 - val_loss: nan - val_accuracy: 0.9686\nEpoch 6/10\n7157/7157 [==============================] - 33s 5ms/step - loss: nan - accuracy: 0.9684 - val_loss: nan - val_accuracy: 0.9686\nEpoch 7/10\n7157/7157 [==============================] - 33s 5ms/step - loss: nan - accuracy: 0.9684 - val_loss: nan - val_accuracy: 0.9686\nEpoch 8/10\n7157/7157 [==============================] - 33s 5ms/step - loss: nan - accuracy: 0.9684 - val_loss: nan - val_accuracy: 0.9686\nEpoch 9/10\n7157/7157 [==============================] - 33s 5ms/step - loss: nan - accuracy: 0.9684 - val_loss: nan - val_accuracy: 0.9686\nEpoch 10/10\n7157/7157 [==============================] - 33s 5ms/step - loss: nan - accuracy: 0.9684 - val_loss: nan - val_accuracy: 0.9686\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Due to the poor performance of the above models (recall for the positive class is low), I will attempt to generate synthetic data to reduce some of the data bias (data is heavily skewed towards the negative class).","metadata":{}},{"cell_type":"code","source":"def marginals(df: pd.DataFrame, col: str) -> {}:\n    \"\"\"\n    maps the probabilty of an occurence to the occurence\n    \"\"\"\n    data = df[col].value_counts()\n    results = [x for x in data]\n    labels = df[col].value_counts().index.to_list()\n    syn_rep = {}\n    \n    for x in range(len(labels)):\n        syn_rep[labels[x]] = max(0, results[x])\n    \n    total = sum(syn_rep.values())\n    \n    marginal = {}\n    for x in labels:\n        marginal[x] = syn_rep[x] / total\n    return marginal\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-11T14:11:18.559192Z","iopub.execute_input":"2024-04-11T14:11:18.559758Z","iopub.status.idle":"2024-04-11T14:11:18.566726Z","shell.execute_reply.started":"2024-04-11T14:11:18.559731Z","shell.execute_reply":"2024-04-11T14:11:18.565816Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def gen_synthetic(x_data: pd.DataFrame, n: int, y_data: pd.Series, syn_type: int):\n    \"\"\"\n    x: X_train\n    n: number of entries to generate\n    y: y_train\n    syn_type: class to generate, either 0 or 1\n    \"\"\"\n    \n    \n    x_data = x_data.assign(target=y_data.values)\n    x_data = x_data[x_data['target'] == syn_type]\n    x_data = x_data.drop('target', axis = 1)\n    \n    syn_y = []\n\n    syn_data = {}\n    for col in x_data.columns.to_list():\n        syn_data[col] = []\n        data = x_data[col].value_counts().index.to_list()\n        marginal = list(marginals(x_data, col).values())\n        synthetic = np.random.choice(data, size=n, p=marginal)\n        \n        for syn_data_point in synthetic:\n            syn_data[col].append(syn_data_point)\n    \n    for i in range(n):\n        syn_y.append(1)\n        \n    syn_x = pd.DataFrame.from_dict(syn_data)\n    syn_y = pd.Series(syn_y)\n    return syn_x, syn_y\n","metadata":{"execution":{"iopub.status.busy":"2024-04-11T14:11:18.567906Z","iopub.execute_input":"2024-04-11T14:11:18.568187Z","iopub.status.idle":"2024-04-11T14:11:18.577899Z","shell.execute_reply.started":"2024-04-11T14:11:18.568163Z","shell.execute_reply":"2024-04-11T14:11:18.577116Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"syn_x, syn_y = gen_synthetic(x_data = filtered_X_train, n = 50000, y_data = y_train, syn_type = 1)\nsyn_x","metadata":{"execution":{"iopub.status.busy":"2024-04-11T14:11:18.578876Z","iopub.execute_input":"2024-04-11T14:11:18.579107Z","iopub.status.idle":"2024-04-11T14:11:19.042832Z","shell.execute_reply.started":"2024-04-11T14:11:18.579086Z","shell.execute_reply":"2024-04-11T14:11:19.041817Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"       avgoutstandbalancel6m_4187114A  downpmt_116A  \\\n0                        2.713450e+18  0.000000e+00   \n1                        2.282517e+18  1.246358e+18   \n2                        2.282517e+18  0.000000e+00   \n3                        2.282517e+18  0.000000e+00   \n4                        2.282517e+18  0.000000e+00   \n...                               ...           ...   \n49995                    1.621284e+19  0.000000e+00   \n49996                    1.233206e+19  0.000000e+00   \n49997                    2.282517e+18  0.000000e+00   \n49998                    2.282517e+18  0.000000e+00   \n49999                    2.282517e+18  0.000000e+00   \n\n       lastrejectcredamount_222A  lastrejectreason_759M  maxannuity_159A  \\\n0                   2.282517e+18           1.540399e+19     6.393138e+17   \n1                   5.599058e+18           1.618013e+19     2.282517e+18   \n2                   1.167478e+19           4.097617e+18     3.599393e+18   \n3                   1.519001e+19           1.540399e+19     1.496651e+19   \n4                   2.282517e+18           1.217869e+19     1.433586e+19   \n...                          ...                    ...              ...   \n49995               1.021877e+19           1.618013e+19     3.020367e+18   \n49996               7.783145e+18           1.540399e+19     1.531098e+19   \n49997               6.933089e+18           1.217869e+19     7.034326e+18   \n49998               2.282517e+18           1.618013e+19     3.521165e+18   \n49999               1.484492e+19           1.540399e+19     8.126718e+18   \n\n       totaldebt_9A  totinstallast1m_4525188A  education_88M  \n0      0.000000e+00              3.142260e+18   1.540399e+19  \n1      4.676800e+18              2.282517e+18   1.540399e+19  \n2      0.000000e+00              2.282517e+18   1.540399e+19  \n3      0.000000e+00              1.222332e+19   1.540399e+19  \n4      0.000000e+00              2.282517e+18   1.540399e+19  \n...             ...                       ...            ...  \n49995  0.000000e+00              2.282517e+18   1.540399e+19  \n49996  1.359709e+19              2.282517e+18   1.540399e+19  \n49997  0.000000e+00              1.784713e+19   1.540399e+19  \n49998  1.199884e+19              2.282517e+18   1.844674e+19  \n49999  0.000000e+00              4.058265e+18   1.540399e+19  \n\n[50000 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>avgoutstandbalancel6m_4187114A</th>\n      <th>downpmt_116A</th>\n      <th>lastrejectcredamount_222A</th>\n      <th>lastrejectreason_759M</th>\n      <th>maxannuity_159A</th>\n      <th>totaldebt_9A</th>\n      <th>totinstallast1m_4525188A</th>\n      <th>education_88M</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.713450e+18</td>\n      <td>0.000000e+00</td>\n      <td>2.282517e+18</td>\n      <td>1.540399e+19</td>\n      <td>6.393138e+17</td>\n      <td>0.000000e+00</td>\n      <td>3.142260e+18</td>\n      <td>1.540399e+19</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.282517e+18</td>\n      <td>1.246358e+18</td>\n      <td>5.599058e+18</td>\n      <td>1.618013e+19</td>\n      <td>2.282517e+18</td>\n      <td>4.676800e+18</td>\n      <td>2.282517e+18</td>\n      <td>1.540399e+19</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.282517e+18</td>\n      <td>0.000000e+00</td>\n      <td>1.167478e+19</td>\n      <td>4.097617e+18</td>\n      <td>3.599393e+18</td>\n      <td>0.000000e+00</td>\n      <td>2.282517e+18</td>\n      <td>1.540399e+19</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.282517e+18</td>\n      <td>0.000000e+00</td>\n      <td>1.519001e+19</td>\n      <td>1.540399e+19</td>\n      <td>1.496651e+19</td>\n      <td>0.000000e+00</td>\n      <td>1.222332e+19</td>\n      <td>1.540399e+19</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.282517e+18</td>\n      <td>0.000000e+00</td>\n      <td>2.282517e+18</td>\n      <td>1.217869e+19</td>\n      <td>1.433586e+19</td>\n      <td>0.000000e+00</td>\n      <td>2.282517e+18</td>\n      <td>1.540399e+19</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49995</th>\n      <td>1.621284e+19</td>\n      <td>0.000000e+00</td>\n      <td>1.021877e+19</td>\n      <td>1.618013e+19</td>\n      <td>3.020367e+18</td>\n      <td>0.000000e+00</td>\n      <td>2.282517e+18</td>\n      <td>1.540399e+19</td>\n    </tr>\n    <tr>\n      <th>49996</th>\n      <td>1.233206e+19</td>\n      <td>0.000000e+00</td>\n      <td>7.783145e+18</td>\n      <td>1.540399e+19</td>\n      <td>1.531098e+19</td>\n      <td>1.359709e+19</td>\n      <td>2.282517e+18</td>\n      <td>1.540399e+19</td>\n    </tr>\n    <tr>\n      <th>49997</th>\n      <td>2.282517e+18</td>\n      <td>0.000000e+00</td>\n      <td>6.933089e+18</td>\n      <td>1.217869e+19</td>\n      <td>7.034326e+18</td>\n      <td>0.000000e+00</td>\n      <td>1.784713e+19</td>\n      <td>1.540399e+19</td>\n    </tr>\n    <tr>\n      <th>49998</th>\n      <td>2.282517e+18</td>\n      <td>0.000000e+00</td>\n      <td>2.282517e+18</td>\n      <td>1.618013e+19</td>\n      <td>3.521165e+18</td>\n      <td>1.199884e+19</td>\n      <td>2.282517e+18</td>\n      <td>1.844674e+19</td>\n    </tr>\n    <tr>\n      <th>49999</th>\n      <td>2.282517e+18</td>\n      <td>0.000000e+00</td>\n      <td>1.484492e+19</td>\n      <td>1.540399e+19</td>\n      <td>8.126718e+18</td>\n      <td>0.000000e+00</td>\n      <td>4.058265e+18</td>\n      <td>1.540399e+19</td>\n    </tr>\n  </tbody>\n</table>\n<p>50000 rows Ã— 8 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"syn_X_train = pd.concat([syn_x, filtered_X_train.head(50000)])\nsyn_y_train = pd.concat([syn_y, y_train.head(50000)])","metadata":{"execution":{"iopub.status.busy":"2024-04-11T14:11:19.044211Z","iopub.execute_input":"2024-04-11T14:11:19.044599Z","iopub.status.idle":"2024-04-11T14:11:19.054273Z","shell.execute_reply.started":"2024-04-11T14:11:19.044567Z","shell.execute_reply":"2024-04-11T14:11:19.053253Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"layers = [\n    Flatten(),\n    Dense(500,\n          activation = \"relu\"),\n    Dense(500,\n          activation = \"relu\"),\n    Dense(500,\n          activation = \"relu\"),\n    Dense(500,\n          activation = \"relu\"),\n    Dense(500,\n          activation = \"relu\"),\n    Dense(500,\n          activation = \"relu\"),\n    Dense(500,\n          activation = \"relu\"),\n    Dense(500,\n          activation = \"relu\"),\n    Dense(500,\n          activation = \"relu\"),\n    Dense(500,\n          activation = \"relu\"),\n    Dense(500,\n          activation = \"relu\"),\n    Dense(500,\n          activation = \"relu\"),\n    Dense(500,\n          activation = \"relu\"),\n    Dense(500,\n          activation = \"relu\"),\n    Dense(500,\n          activation = \"relu\"),\n    Dense(500,\n          activation = \"relu\"),\n    Dense(500,\n          activation = \"relu\"),\n    Dense(500,\n          activation = \"relu\"),\n    Dense(500,\n          activation = \"relu\"),\n    Dense(500,\n          activation = \"relu\"),\n    Dense(10,\n          activation = \"softmax\")\n]\n\nmodel = Sequential(layers)\n\n#refer to https://www.tensorflow.org/tutorials/keras/classification tutorial to check on how to use compile function\nmodel.compile(optimizer='SGD', loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n\n# This will start the training and save each epoch output in the history list.\nhistory = model.fit(syn_X_train, syn_y_train, batch_size=256, epochs=10 , validation_data=(filtered_X_valid, y_valid))","metadata":{"execution":{"iopub.status.busy":"2024-04-11T14:11:19.055556Z","iopub.execute_input":"2024-04-11T14:11:19.056355Z","iopub.status.idle":"2024-04-11T14:12:05.623527Z","shell.execute_reply.started":"2024-04-11T14:11:19.056299Z","shell.execute_reply":"2024-04-11T14:12:05.622661Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1712844682.113989     128 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"391/391 [==============================] - 7s 12ms/step - loss: nan - accuracy: 0.4838 - val_loss: nan - val_accuracy: 0.9686\nEpoch 2/10\n391/391 [==============================] - 4s 11ms/step - loss: nan - accuracy: 0.4838 - val_loss: nan - val_accuracy: 0.9686\nEpoch 3/10\n391/391 [==============================] - 4s 11ms/step - loss: nan - accuracy: 0.4838 - val_loss: nan - val_accuracy: 0.9686\nEpoch 4/10\n391/391 [==============================] - 4s 11ms/step - loss: nan - accuracy: 0.4838 - val_loss: nan - val_accuracy: 0.9686\nEpoch 5/10\n391/391 [==============================] - 4s 11ms/step - loss: nan - accuracy: 0.4838 - val_loss: nan - val_accuracy: 0.9686\nEpoch 6/10\n391/391 [==============================] - 4s 11ms/step - loss: nan - accuracy: 0.4838 - val_loss: nan - val_accuracy: 0.9686\nEpoch 7/10\n391/391 [==============================] - 4s 11ms/step - loss: nan - accuracy: 0.4838 - val_loss: nan - val_accuracy: 0.9686\nEpoch 8/10\n391/391 [==============================] - 5s 12ms/step - loss: nan - accuracy: 0.4838 - val_loss: nan - val_accuracy: 0.9686\nEpoch 9/10\n391/391 [==============================] - 4s 11ms/step - loss: nan - accuracy: 0.4838 - val_loss: nan - val_accuracy: 0.9686\nEpoch 10/10\n391/391 [==============================] - 4s 11ms/step - loss: nan - accuracy: 0.4838 - val_loss: nan - val_accuracy: 0.9686\n","output_type":"stream"}]},{"cell_type":"code","source":"ds = tf.data.Dataset.from_tensor_slices(filtered_X_test)\nds_y = tf.data.Dataset.from_tensor_slices(y_test)\n\nnn_results = model.predict(filtered_X_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T14:31:44.040019Z","iopub.execute_input":"2024-04-11T14:31:44.040403Z","iopub.status.idle":"2024-04-11T14:32:55.575055Z","shell.execute_reply.started":"2024-04-11T14:31:44.040374Z","shell.execute_reply":"2024-04-11T14:32:55.573891Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"28625/28625 [==============================] - 52s 2ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"nn_results","metadata":{"execution":{"iopub.status.busy":"2024-04-11T14:33:26.621551Z","iopub.execute_input":"2024-04-11T14:33:26.621884Z","iopub.status.idle":"2024-04-11T14:33:26.629375Z","shell.execute_reply.started":"2024-04-11T14:33:26.621859Z","shell.execute_reply":"2024-04-11T14:33:26.628411Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"classification_results = pd.DataFrame(classification_report(y_train, nn_results, output_dict=True)).transpose()\nclassification_results","metadata":{"execution":{"iopub.status.busy":"2024-04-11T14:33:16.447046Z","iopub.execute_input":"2024-04-11T14:33:16.448187Z","iopub.status.idle":"2024-04-11T14:33:16.763923Z","shell.execute_reply.started":"2024-04-11T14:33:16.448153Z","shell.execute_reply":"2024-04-11T14:33:16.762677Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/utils/multiclass.py:123: RuntimeWarning: invalid value encountered in cast\n  return y.dtype.kind == \"f\" and np.all(y.astype(int) == y)\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/multiclass.py:380: RuntimeWarning: invalid value encountered in cast\n  if xp.any(data != data.astype(int)):\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m classification_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[1;32m      2\u001b[0m classification_results\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2310\u001b[0m, in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassification_report\u001b[39m(\n\u001b[1;32m   2196\u001b[0m     y_true,\n\u001b[1;32m   2197\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2205\u001b[0m ):\n\u001b[1;32m   2206\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m \n\u001b[1;32m   2208\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <classification_report>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2307\u001b[0m \u001b[38;5;124;03m    <BLANKLINE>\u001b[39;00m\n\u001b[1;32m   2308\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2310\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2312\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2313\u001b[0m         labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:88\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     86\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m     87\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 88\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_pred\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m y_type \u001b[38;5;241m=\u001b[39m {type_true, type_pred}\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/multiclass.py:381\u001b[0m, in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name)\u001b[0m\n\u001b[1;32m    379\u001b[0m     data \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mif\u001b[39;00m issparse(y) \u001b[38;5;28;01melse\u001b[39;00m y\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39many(data \u001b[38;5;241m!=\u001b[39m data\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)):\n\u001b[0;32m--> 381\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinuous\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m suffix\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# Check multiclass\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n","\u001b[0;31mValueError\u001b[0m: Input y_pred contains NaN."],"ename":"ValueError","evalue":"Input y_pred contains NaN.","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}